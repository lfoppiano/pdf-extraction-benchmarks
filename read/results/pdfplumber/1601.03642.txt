1
Creativity in Machine Learning w
x 0
0
w
x 1
1
w
Martin Thoma x2 2 Σ ϕ
w
E-Mail: info@martin-thoma.de x 3
.3
.. wn
x
n
Abstract—Recent machine learning techniques can be modiﬁed (a) Exampleofanartiﬁcialneuronunit.(b) Avisualizationofasimplefeed-
to produce creative results. Those results did not exist before; it xiaretheinputsignalsandwiare forwardneuralnetwork.The5in-
is not a trivial combination of the data which was fed into the weightswhichhavetogetlearned. putnodesarered,the2biasnodes
machine learning system. The obtained results come in multiple Each input signal gets multiplied are gray, the 3 hidden units are
forms: As images, as text and as audio. with its weight, everything gets greenandthesingleoutputnode
summedupandtheactivationfunc- isblue.
This paper gives a high level overview of how they are created tionϕisapplied.
and gives some examples. It is meant to be a summary of the
current work and give people who are new to machine learning Fig. 1: Neural networks are based on simple units which get
some starting points. combined to complex networks.
6 I. INTRODUCTION This means that machine learning programs adjust internal
1 parameters to ﬁt the data they are given. Those computer
0 According to [Gad06] creativity is “the ability to use your programs are still developed by software developers, but the
2 imagination to produce new ideas, make things etc.” and developer writes them in a way which makes it possible to
 
n imagination is “the ability to form pictures or ideas in your adjust them without having to re-program everything. Machine
a mind”. learning programs should generally improve when they are fed
J
with more data.
2  Recentadvancesinmachinelearningproduceresultswhichthe
1 author would intuitively call creative. A high-level overview The ﬁeld of machine learning is related to statistics. Some
   over several of those algorithms are described in the following. algorithms directly try to ﬁnd models which are based on well-
]
V known distribution assumptions of the developer, others are
This paper is structured as follows: Section II introduces the
more general.
C reader on a very simple and superﬁcial level to machine
. learning, Section III gives examples of creativity with images, A common misunderstanding of people who are not related
s
c Section IV gives examples of machines producing textual in this ﬁeld is that the developers don’t understand what their
 [ content, and Section V gives examples of machine learning machine learning program is doing. It is understood very well
1  and music. A discussion follows in Section VI. in the sense that the developer, given only a pen, lots of paper
v and a calculator could calculate the same result as the machine
2
doeswhenhegetsthesamedata.Andlotsoftime,ofcourse.It
4 II. BASICSOFMACHINELEARNING isnotunderstoodinthesensethatitishardtomakepredictions
6
3 how the algorithm behaves without actually trying it. However,
The traditional approach of solving problems with software
0 this is similar to expecting from an electrical engineer to
is to program machines to do so. The task is divided in as
. explain how a computer works. The electrical engineer could
1 simple sub-tasks as possible, the subtasks are analyzed and the
0 probably get the knowledge he needs to do so, but the amount
machineisinstructedtoprocesstheinputwithhuman-designed
6 of time required to understand such a complex system from
algorithms to produce the desired output. However, for some
1 basic building blocks is a time-intensive and difﬁcult task.
: taskslikeobjectrecognitionthisapproachisnotfeasible.There
v
are way to many different objects, different lighting situations, An important group of machine learning algorithms was
i
X variations in rotation and the arrangement of a scene for a inspired by biological neurons and are thus called artiﬁcial
r human to think of all of them and model them. But with the neural networks. Those networks are based on mathematical
a
internet, cheap computers, cameras, crowd-sourcing platforms functions called artiﬁcial neurons which take n ∈ N num-
like Wikipedia and lots of Websites, services like Amazon bers x ,...,x ∈ R as input, multiply them with weights
1 n
Mechanical Turk and several other changes in the past decades w ,...,w ∈ R, add them and apply a so called activation
1 n
alotofdatahasbecomeavailable.Theideaofmachinelearning function ϕ as visualized in Figure 1(a). One example of such
is to make use of this data. an activation function is the sigmoid function ϕ(x)= 1 .
1+e−x
Those functions act as building blocks for more complex
A formal deﬁnition of the ﬁeld of Machine Learning is given
systems as they can be chained and grouped in layers as
by Tom Mitchel [Mit97]:
visualized in Figure 1(b). The interesting question is how
A computer program is said to learn from experi- the parameters w are learned. This is usually done by an
i
ence E with respect to some class of tasks T and optimization technique called gradient descent. The gradient
performance measure P, if its performance at tasks descent algorithm takes a function which has to be derivable,
inT,asmeasuredbyP,improveswithexperienceE. starts at any point of the surface of this error function and
2
makes a step in the direction which goes downwards. Hence
it tries to ﬁnd a minimum of this high-dimensional function.
There is, of course, a lot more to say about machine learning.
Theinterestedreadermightwanttoreadtheintroductiongiven
by Mitchell [Mit97].
III. IMAGEDATA
Applying a simple neural network on image data directly can
work, but the number of parameters gets extraordinary large.
One would take one neuron per pixel and channel. This means
for 500px×500px RGB images one would get 750,000 input
signals. To approach this problem, so called Convolutional
Neural Networks (CNNs) were introduced. Instead of learning
the full connection between the input layer and the ﬁrst Fig. 2: Aurelia aurita
hidden layer, those networks make use of convolution layers.
Convolution layers learn a convolution; this means they learn
the weights of an image ﬁlter. An additional advantage is that
CNNs make use of spacial relationships of the pixels instead
of ﬂattening the image to a stream of single numbers.
An excellent introduction into CNNs is given by [Nie15].
A. Google DeepDream
The gradient descent algorithm which optimizes most of the
parametersinneuralnetworksiswell-understood.However,the
effect it has on the recognition system is difﬁcult to estimate.
[MOT15] proposes a technique to analyze the weights learned
by such a network. A similar idea was applied by [VKMT13].
For example, consider a neural network which was trained to
recognize various images like bananas. This technique turns Fig. 3: DeepDream impression of Aurelia aurita
the network upside down and starts with random noise. To
analyze what the network considers bananas to look like, the
random noise image is gradually tweaked so that it generates Ithasbecomefamousintheinternet[Red].Usually,theimages
theoutput“banana”.Additionally,thechangescanberestricted are generated in iterations and in each iteration it is zoomed
inawaythatthestatisticsoftheinputimagehavetobesimilar into the image.
to natural images. One example of this is that neighboring Images and videos published by the Google engineers can be
pixels are correlated. seenat[goo15].Figure2showstheoriginalimagefromwhich
Figure 3 was created with the deep dream algorithm.
Another technique is to amplify the output of layers. This was
described in [MOT15]:
Weaskthenetwork:“Whateveryouseethere,Iwant
B. Artistic Style Imitation
more of it!” This creates a feedback loop: if a cloud
looks a little bit like a bird, the network will make
it look more like a bird. This in turn will make the A key idea of neural networks is that they learn different
network recognize the bird even more strongly on representations of the data in each layer. In the case of
the next pass and so forth, until a highly detailed CNNs, this can easily be visualized as it was done in various
bird appears, seemingly out of nowhere. papers [ZF14]. Usually, one ﬁnds that the network learned
to build edge detectors in the ﬁrst layer and more complex
The name “Inceptionism” in the title of [MOT15] comes from
structures in the upper layers.
the science-ﬁction movie “Inception” (2010). One reason it
might be chosen is because neural networks are structured Gatys,EckerandBethgeshowedin[GEB15]thatwithaclever
in layers. Recent publications tend to have more and more choice of features it is possible to separate the general style of
layers [HZRS15]. The used jargon is to say they get “deeper”. an image in terms of local image appearance from the content
As this technique as published by Google engineers, the of an image. They support their claim by applying the style of
technique is called Google DeepDream. different artists to an arbitrary image of their choice.
3
IV. TEXTDATA
Digital text is the ﬁrst form of natural communication which
involved computers. It is used in the form of chats, websites,
on collaborative projects like Wikipedia, in scientiﬁc literature.
Of course, it was used in pre-digital times, too: In newspaper,
in novels, in dramas, in religious texts like the bible, in books
for education, in notes from conversations.
(a) OriginalImage (b) Styleimage This list could be continued and most of these kinds of texts
are now available in digital form. This digital form can be
used to teach machines to generate similar texts.
The most simple language model which is of use is an n-gram
model. This model makes use of sequences of the length n to
model language. It can be used to get the probability of a third
word, given the previous two words. This way, a complete text
can be generated word by word. Reﬁnements and extensions
to this model are discussed in the ﬁeld of Natural Language
Processing (NLP).
However, there are much more sophisticated models. One
of those are character predictors based on Recurrent Neural
Networks (RNNs). Those character predictors take a sequence
of characters as input and predict the next character. In that
sense they are similar to the n-gram model, but operate on
a lower level. Using such a predictor, one can generate texts
(c) TheartisticstyleofVanGogh’s“StarryNight”appliedtothephotograph
ofaScottishHighlandCattle. character by character. If the model is good, the text can have
the correct punctuation. This would not be possible with a
Fig. 4: The algorithm takes both, the original image and the word predictor.
style image to produce the result.
Character predictors can be implemented with RNNs. In con-
trast to standard feed-forward neural networks like multilayer
This artistic style imitation can be seen itself as creative work. Perceptrons (MLPs) which was shown in Figure 1(b), those
An example is given by Figure 4. The code which created this networksaretrainedtotaketheiroutputatsomepointaswellas
example is available under [Joh16]. the normal input. This means they can keep some information
over time. One of the most common variant to implement
Something similar was done by [SPB+14], where the style of RNNs is by using so called Long short-term memory (LSTM)
a portrait photograph was transferred to another photograph. cells [HS97].
A demo can be seen on [Shi14].
Recurrentnetworksapplytwomainideasinordertolearn:The
ﬁrst is called unrolling and means that an recurrent network
is imagined to be an inﬁnite network over time. At each time
C. Drawing Robots
step the recurrent neurons get duplicated. The second idea is
weight sharing which means that those unrolled neurons share
PatrickTressetandFrdricFolLeymariecreatedasystemcalled
the same weight.
AIKON (Automatic IKONic drawing) which can automatically
generated sketches for portraits [TL05]. AIKON takes a digital
photograph, detects faces on them and sketches them with a
A. Similar Texts Generation
pen-plotter.
Tresset and Leymaire use k-means clustering [KMN+02] to KarpathytrainedmultiplecharacterRNNsondifferentdatasets
segment regions of the photograph with similar color which, and gave an excellent introduction [Kar15b]. He trained it on
in turn, will get a similar shading. Paul Graham’s essays, all the works of Shakespeare, the Hutter
Prize [hut] 100MB dataset of raw Wikipedia articles, the raw
Such a drawing robot could apply machine learning techniques
LATEXsourceﬁleofabookaboutalgebraicstacksandgeometry
known from computer vision for detecting the human. It
and Linux C code.
could apply self-learning techniques to draw results most
similar to the artists impression of the image. However, the With that training data, the models can generate similar texts.
system described in [TL05] seems not to be a machine New works which look like Shakespeare plays, new Wikipedia
learning computer program according to the deﬁnition by Tom articles, new Linux code and new papers about algebraic
Mitchell [Mit97]. geometry can thus automatically be generated. At a ﬁrst
4
glance, they do look authentic. The syntax was mostly used we will now investigate the work which was done in audio
correctly, the formatting looks as expected, the sentences are synthesization.
grammaticallycorrect.However,whenonelooksatthebroader
context it is easy to recognize that the algorithm has no insight
A. Emily Howell
in what it is doing. It does match patterns really well, but it
fails to follow a central theme. In the context of C code this
David Cope created a project called “Experiments in Musical
means that new variables are introduced, but not used. At the
Intelligence” (short: EMI or Emmy) in 1984 [Cop87]. He
same time, variables which were not declared are used. In
introduces the idea of seeing music as a language which
the context of Shakespear plays this means that a lot of new
can be analyzed with natural language processing (NLP)
characters are introduced, but they don’t speak with each other
methods. Cope mentions that EMI was more useful to him,
or about each other.
when he used the system to “create small phrase-size textures
The code used to generate these examples is available and as next possibilities using its syntactic dictionary and rule
ready to use through [Kar15a]. A couple of examples are base” [Cop87].
in Section A.
In 2003, Cope started a new project which was based on EMI:
Emily Howell [Cop13]. This program is able to “creat[e] both
highly authentic replications and novel music compositions”.
B. Chatbots
Thereadermightwanttolistento[Cop12]togetanimpression
of the beauty of the created music.
Chatbots are computer programs which participate in chat
rooms as autonomous agents. This means they have similar According to Cope, an essential part of music is “a set of
permissions and possibilities as usual human users have, but instructions for creating different, but highly related self-
users can trigger a set of commands to make the bot give them replications”. Emmy was programmed to ﬁnd this set of
valuable information or features. instructions. It tries to ﬁnd the “signature” of a composer,
which Cope describes as “contiguous patterns that recur in two
A special category of chatbots are such bots which actively
or more works of the composer”.
participate in the conversation, which is usually not the case.
One of the earliest programs in this category is ELIZA, a bot The new feature of Emily Howell compared to Emmy is that
created by Joseph Weizenbaum in the 1960s [Wei76]. This Emily Howell does not necessarily remain in a single, already
program had a set of patterns implemented to which it would known style.
reply in a seemingly smart way in a psychologists fashion.
Emily Howell makes use of association network. Cope empha-
This means quite often the program would simply repeat the
sizes that this is not a form of a neural network. However, it
last sentence and add something meaningless like “How do
is not clear from [Cop13] how exactly an association network
you feel about it?”. According to [Cur14], Weizenbaum once
is trained. Cope mentions that Emily Howell is explained in
found his secretary — who was aware of the fact that this is
detail in [Cop05].
a computer program — chatting with the machine. When he
looked over her shoulder, she asked him “would you mind
leaving the room”.
B. GRUV
Today, much more sophisticated chatbots exist. They make use
Recurrent neural networks — LSTM networks, to be exact
of the vast amount of data which is available by millions of
— are used in [NV15] together with Gated Recurrent Units
Wikipediaarticles,chatprotocols,websites,helpdeskprotocols,
(GRU) to build a network which can be trained to generate
subtitles of movies as well as the astonishing increase in
music. Instead of taking notes directly or MIDI ﬁles, Nayebi
computing power to train RNNs and language models similar
and Vitelli took raw audio waveforms as input. Those audio
to the ones described before.
waveformsarefeaturevectorsgivenfortimesteps0,1,...,t−
Interesting results like the following were obtained by [VL15]: 1,t. The network is given those feature vectors X ,...,X
1 t
and has to predict the following feature vector X . This
Human: what is the purpose of life ? t+1
means it continues the music. As the input is continuous, the
Machine: to serve the greater good .
problem was modeled as a regression task. Discrete Fourier
Human: what is the purpose of living ?
Transformation (DFT) was used on chunks of length N of the
Machine: to live forever .
music to obtain features in the frequency domain.
Animplementationcanbefoundat[VN15]andademonstration
V. AUDIODATA can be found at [Vit15].
Common machine learning tasks which involve audio data
are speech recognition, speaker identiﬁcation, identiﬁcation of C. Audio Synthesization
songs. This leads to some less-common, but interesting topics:
The composition of music, the synthesizing of audio as art. Audio synthesization is generating new audio ﬁles. This can
While the composition might be considered in Section IV, eitherbemusicorspeech.Withthetechniquesdescribedbefore,
5
neural networks can be trained to generate music note by note. [Joh15a] D. Johnson, “Biaxial recurrent neural network for music
However, it is desirable to allow multiple notes being played composition,” GitHub, Aug. 2015. [Online]. Available: https:
//github.com/hexahedria/biaxial-rnn-music-composition
at the same time.
[Joh15b] ——, “Composing music with recurrent neu-
ThisideaandsomeotherswereappliedbyDanielJohnson.He ral networks,” Personal Blog, Aug. 2015. [On-
wrote a very good introduction into neural networks for music line]. Available: http://www.hexahedria.com/2015/08/03/
composing-music-with-recurrent-neural-networks/
composition which explains those ideas [Joh15b]. Example
compositionsareavailablethere,too.Healsomadethecodefor [Joh16] J.Johnson,“neural-style,”GitHub,Jan.2016.[Online].Available:
https://github.com/jcjohnson/neural-style
hisBiaxialRecurrentNeuralNetworkavailableunder[Joh15a].
[Kar15a] A.Karpathy,“char-rnn,”GitHub,Nov.2015.[Online].Available:
https://github.com/karpathy/char-rnn
VI. DISCUSSION [Kar15b] ——, “The unreasonable effectiveness of recurrent neural
networks,” Personal Blog, May 2015. [Online]. Available:
What does these examples mean for our understanding of http://karpathy.github.io/2015/05/21/rnn-effectiveness/
creativity? Does it inﬂuence how much we value art? Could [KMN+02] T.Kanungo,D.Mount,N.Netanyahu,C.Piatko,R.Silverman,
wedeﬁneartandcreativitybetterafterhavingthoseandsimilar andA.Wu,“Anefﬁcientk-meansclusteringalgorithm:analysis
andimplementation,”PatternAnalysisandMachineIntelligence,
results?
IEEETransactionson,vol.24,no.7,pp.881–892,Jul2002.
I think we might readjust our understanding of creativity just [Mit97] T. M. Mitchell, Machine learning, ser. McGraw Hill series in
like we adjusted our understanding of algorithmically hard computerscience. McGraw-Hill,1997.
problems after Deep Blue won against the reigning world [MOT15] A. Mordvintsev, C. Olah, and M. Tyka, “Inceptionism: Going
chess champion Garry Kasparov in 1997. deeper into neural networks,” googleresearch.blogspot.co.uk,
Jun.2015.[Online].Available:http://googleresearch.blogspot.de/
However,bynowitisobviousthatmachinelearningalgorithms 2015/06/inceptionism-going-deeper-into-neural.html
cannot compete with human artists. Today’s state of the art [Nie15] M. A. Nielsen, Neural Networks and Deep Learn-
algorithms which are purely based on machine learning don’t ing. Determination Press, 2015. [Online]. Avail-
able: http://neuralnetworksanddeeplearning.com/chap6.html#
follow a central theme. They lack the ability to plan. Although
introducing convolutional networks
clever algorithms were implemented for composing music, it
[NV15] A.NayebiandM.Vitelli,“GRUV:Algorithmicmusicgeneration
seems as if there is still a lot of supervision involved.
using recurrent neural networks,” 2015. [Online]. Available:
http://cs224d.stanford.edu/reports/NayebiAran.pdf
REFERENCES [Red] “Deepdream,” Reddit. [Online]. Available: https://www.reddit.
com/r/deepdream/
[Cop87] D. Cope, “Experiments in music intelligence (emi),” 1987. [Shi14] Y. Shih, “Style transfer for headshot portraits,” YouTube, Jun.
[Online].Available:http://hdl.handle.net/2027/spo.bbp2372.1987. 2014. [Online]. Available: https://www.youtube.com/watch?v=
025 Hj5lGFzlubU
[Cop05] ——, Computer models of musical creativity. MIT Press [SPB+14] Y. Shih, S. Paris, C. Barnes, W. T. Freeman, and F. Durand,
Cambridge,2005. “Style transfer for headshot portraits,” ACM Transactions on
Graphics(TOG),vol.33,no.4,p.148,2014.[Online].Available:
[Cop12] ——, “Emily howell fugue,” YouTube, Oct. 2012. [Online]. http://dl.acm.org/citation.cfm?id=2601137
Available:https://www.youtube.com/watch?v=jLR- c uCwI
[TL05] P.TressetandF.F.Leymarie,“Generativeportraitsketching,”in
[Cop13] ——,“Thewell-programmedclavier:Styleincomputermusic ProceedingsofVSMM,2005,pp.739–748.
composition,” XRDS: Crossroads, The ACM Magazine for
Students, vol. 19, no. 4, pp. 16–20, 2013. [Online]. Available: [Vit15] M. Vitelli, “Algorithmic music generation with recurrent
http://dl.acm.org/citation.cfm?id=2460444 neural networks,” YouTube, Jun. 2015. [Online]. Available:
https://youtu.be/0VTI1BBLydE
[Cur14] A. Curtis, “Now then,” BBC, Jul. 2014. [On-
line].Available:http://www.bbc.co.uk/blogs/adamcurtis/entries/ [VKMT13] C. Vondrick, A. Khosla, T. Malisiewicz, and A. Torralba,
78691781-c9b7-30a0-9a0a-3ff76e8bfe58 “Hoggles: Visualizing object detection features,” in Computer
Vision(ICCV),2013IEEEInternationalConferenceon. IEEE,
[Gad06] A.Gadsby,Ed.,DictionaryofContemporaryEnglish. Pearson 2013, pp. 1–8. [Online]. Available: http://ieeexplore.ieee.org/
EducationLimited,2006. xpls/abs all.jsp?arnumber=6751109
[GEB15] L.A.Gatys,A.S.Ecker,andM.Bethge,“Aneuralalgorithmof [VL15] O. Vinyals and Q. Le, “A neural conversational model,”
artisticstyle,”arXivpreprintarXiv:1508.06576,2015.[Online]. arXivpreprintarXiv:1506.05869,Jul.2015.[Online].Available:
Available:http://arxiv.org/abs/1508.06576 http://arxiv.org/abs/1506.05869v2
[goo15] “Inceptionism: Going deeper into neural networks,” Google [VN15] M. Vitelli and A. Nayebi, “GRUV,” Aug. 2015. [Online].
Photos,Jun.2015.[Online].Available:https://goo.gl/Bydofw Available:https://github.com/MattVitelli/GRUV
[HS97] S.HochreiterandJ.Schmidhuber,“Longshort-termmemory,” [Wei76] J. Weizenbaum, Computer Power and Human Reason: From
Neural computation, vol. 9, no. 8, pp. 1735–1780, 1997. JudgementtoCalculation. W.H.Freeman&CoLtd,1976.
[Online].Available:http://ieeexplore.ieee.org/xpl/freeabs all.jsp?
arnumber=6795963 [ZF14] M.D.ZeilerandR.Fergus,“Visualizingandunderstandingcon-
volutionalnetworks,”inComputerVision–ECCV2014. Springer,
[hut] “50’000europrizeforcompressinghumanknowledge.”[Online]. 2014,pp.818–833.
Available:http://prize.hutter1.net/
[HZRS15] K.He,X.Zhang,S.Ren,andJ.Sun,“Deepresiduallearning
forimagerecognition,”arXivpreprintarXiv:1512.03385,2015.
[Online].Available:http://arxiv.org/abs/1512.03385
6
APPENDIXA
AUTOMATICALLYGENERATEDTEXTS
The following texts were generated by [Kar15a] and published by Karpathy on [Kar15b]. This is meant to be a copy for
convenience and the case that the website is not reachable.
A. Shakespeare
PANDARUS:
Alas, I think he shall be come approached and the day
When little srain would be attain’d into being never fed,
And who is but a chain and subjects of his death,
I should not sleep.
Second Senator:
They are away this miseries, produced upon my soul,
Breaking and strongly should be buried, when I perish
The earth and thoughts of many states.
DUKE VINCENTIO:
Well, your wit is in the care of side and that.
Second Lord:
They would be ruled after this chamber, and
my fair nues begun out of the fact, to be conveyed,
Whose noble souls I’ll have the heart of the wars.
Clown:
Come, sir, I will make did behold your worship.
VIOLA:
I’ll drink it.
B. Wikipeda
Naturalism and decision for the majority of Arab countries’ capitalide was grounded
by the Irish language by [[John Clair]], [[An Imperial Japanese Revolt]], associated
with Guangzham’s sovereignty. His generals were the powerful ruler of the Portugal
in the [[Protestant Immineners]], which could be said to be directly in Cantonese
Communication, which followed a ceremony and set inspired prison, training. The
emperor travelled back to [[Antioch, Perth, October 25|21]] to note, the Kingdom
of Costa Rica, unsuccessful fashioned the [[Thrales]], [[Cynth’s Dajoard]], known
in western [[Scotland]], near Italy to the conquest of India with the conflict.
Copyright was the succession of independence in the slop of Syrian influence that
was a famous German movement based on a more popular servicious, non-doctrinal
and sexual power post. Many governments recognize the military housing of the
[[Civil Liberalization and Infantry Resolution 265 National Party in Hungary]],
that is sympathetic to be to the [[Punjab Resolution]]
(PJS)[http://www.humah.yahoo.com/guardian.
cfm/7754800786d17551963s89.htm Official economics Adjoint for the Nazism, Montgomery
was swear to advance to the resources for those Socialism’s rule,
was starting to signing a major tripad of aid exile.]]
C. Linux Code, 1
/*
7
* Increment the size file of the new incorrect UI_FILTER group information
* of the size generatively.
*/
static int indicate_policy(void)
{
int error;
if (fd == MARN_EPT) {
/*
* The kernel blank will coeld it to userspace.
*/
if (ss->segment < mem_total)
unblock_graph_and_set_blocked();
else
ret = 1;
goto bail;
}
segaddr = in_SB(in.addr);
selector = seg / 16;
setup_works = true;
for (i = 0; i < blocks; i++) {
seq = buf[i++];
bpf = bd->bd.next + i * search;
if (fd) {
current = blocked;
}
}
rw->name = "Getjbbregs";
bprm_self_clearl(&iv->version);
regs->new = blocks[(BPF_STATS << info->historidac)] | PFMR_CLOBATHINC_SECONDS << 12;
return segtable;
}
D. Linux Code, 2
/*
* Copyright (c) 2006-2010, Intel Mobile Communications. All rights reserved.
*
* This program is free software; you can redistribute it and/or modify it
* under the terms of the GNU General Public License version 2 as published by
* the Free Software Foundation.
*
* This program is distributed in the hope that it will be useful,
* but WITHOUT ANY WARRANTY; without even the implied warranty of
* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
*
* GNU General Public License for more details.
*
* You should have received a copy of the GNU General Public License
* along with this program; if not, write to the Free Software Foundation,
* Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
*/
#include <linux/kexec.h>
#include <linux/errno.h>
#include <linux/io.h>
#include <linux/platform_device.h>
#include <linux/multi.h>
8
#include <linux/ckevent.h>
#include <asm/io.h>
#include <asm/prom.h>
#include <asm/e820.h>
#include <asm/system_info.h>
#include <asm/setew.h>
#include <asm/pgproto.h>
#define REG_PG vesa_slot_addr_pack
#define PFM_NOCOMP AFSR(0, load)
#define STACK_DDR(type) (func)
#define SWAP_ALLOCATE(nr) (e)
#define emulate_sigs() arch_get_unaligned_child()
#define access_rw(TST) asm volatile("movd %%esp, %0, %3" : : "r" (0)); \
if (__type & DO_READ)
static void stat_PC_SEC __read_mostly offsetof(struct seq_argsqueue, \
pC>[1]);
static void
os_prefix(unsigned long sys)
{
#ifdef CONFIG_PREEMPT
PUT_PARAM_RAID(2, sel) = get_state_state();
set_pid_sum((unsigned long)state, current_state_str(),
(unsigned long)-1->lr_full; low;
}
